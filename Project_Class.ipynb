{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dc404d-f717-49e5-ba92-4e1950d8ad1c",
   "metadata": {},
   "source": [
    "# Classification Model Tuning: Predicting Purchase Behavior  \n",
    "\n",
    "## Project Overview  \n",
    "This project aims to identify the **optimal classification model** for predicting whether users will purchase a product based on demographic and financial features from the `Social_Network_Ads.csv` dataset.  \n",
    "\n",
    "### Dataset Features  \n",
    "| Feature            | Type       | Description                          |  \n",
    "|---------------------|------------|--------------------------------------|  \n",
    "| `User ID`           | Identifier | Unique user identifier (removed)     |  \n",
    "| `Gender`            | Categorical| User's gender (Male/Female)          |  \n",
    "| `Age`               | Numerical  | User's age                           |  \n",
    "| `EstimatedSalary`   | Numerical  | User's estimated annual salary      |  \n",
    "| `Purchased`         | Binary     | Target label (0=Not Purchased, 1=Purchased)|  \n",
    "\n",
    "**Objective**:  \n",
    "1. Compare performance of **Logistic Regression**, **KNN**, and **Naïve Bayes** models.  \n",
    "2. Optimize models via **hyperparameter tuning**.  \n",
    "3. Evaluate using accuracy, precision, recall, and F1-score.  \n",
    "\n",
    "---\n",
    "\n",
    "## Models Explored  \n",
    "### 1. Logistic Regression  \n",
    "- **Algorithm**: Predicts probabilities using a logistic function.  \n",
    "- **Tuned Parameters**:  \n",
    "  - `C` (Inverse regularization strength): [0.01, 0.1, 1, 10, 100]  \n",
    "  - `penalty`: L1/L2 regularization  \n",
    "  - `solver`: Optimization algorithms (`saga`, `liblinear`)  \n",
    "\n",
    "### 2. K-Nearest Neighbors (KNN)  \n",
    "- **Algorithm**: Classifies based on majority vote of k-nearest neighbors.  \n",
    "- **Tuned Parameters**:  \n",
    "  - `n_neighbors`: [2-9]  \n",
    "  - `weights`: `uniform` (equal weighting) vs `distance` (weight by inverse distance)  \n",
    "  - `metric`: Distance metrics (`euclidean`, `manhattan`)  \n",
    "\n",
    "### 3. Naïve Bayes (Gaussian)  \n",
    "- **Algorithm**: Probabilistic classifier using Bayes' theorem with feature independence assumption.  \n",
    "- **No hyperparameter tuning** applied (used as baseline).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a1537-c7ee-4c55-a8d2-16f71ff12498",
   "metadata": {},
   "source": [
    "#### Let's begin with importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e38a519a-3cdf-4d0c-ba0b-ee3648dd955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15373061-de12-4451-90ff-ae242f121ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Social_Network_Ads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4ab5a424-dad0-4ca2-b3de-2acb555d2433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15624510</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15810944</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15668575</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15603246</td>\n",
       "      <td>Female</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15804002</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User ID  Gender  Age  EstimatedSalary  Purchased\n",
       "0  15624510    Male   19            19000          0\n",
       "1  15810944    Male   35            20000          0\n",
       "2  15668575  Female   26            43000          0\n",
       "3  15603246  Female   27            57000          0\n",
       "4  15804002    Male   19            76000          0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d4bb0-1c4d-43f9-b514-b28e8f49332d",
   "metadata": {},
   "source": [
    "## Data preprocessing & Feature Engineering:\n",
    "\n",
    "- Dropped `User ID` (non-predictive).\n",
    "- Encoded `Gender` using **one-hot encoding** (`Gender_Female`, `Gender_Male`).\n",
    "- Feature Scaling\n",
    "  - Applied **StandardScaler** to `Age` and `EstimatedSalary` for normalization.\n",
    "- Data Splitting\n",
    "  - Stratified sampling ensures balanced class distribution in train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22c07281-f108-49c5-bb9d-86880e2f1638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID            0\n",
      "Gender             0\n",
      "Age                0\n",
      "EstimatedSalary    0\n",
      "Purchased          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a7af9-cae1-4a97-b7ba-216378e5a75c",
   "metadata": {},
   "source": [
    "We see, that dataset is fine. It has no empty values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003450e-4aaf-4c8f-a467-66e5f60d6fda",
   "metadata": {},
   "source": [
    "In the opposite case we would handle the miising data in the following way:\n",
    "- Gender -> most frequent gender: `data['Gender'] = data.fillna(data['Gender']. value_counts(). index[0])`\n",
    "- age field -> median value: `data['Age'] = data.fillna(data['Age'].median())`\n",
    "- salary -> mean value: `data['EstimatedSalary'] = data.fillna(data['EstimatedSalary'].median())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "da79ce73-bb47-421e-b5fc-6ff1633410a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = ['User ID'], inplace = True)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n",
    "encoded_arr = encoder.fit_transform(data[['Gender']])\n",
    "feature_names = encoder.get_feature_names_out()\n",
    "encoded_df = pd.DataFrame(encoded_arr, columns = feature_names)\n",
    "data = pd.concat([data.drop(columns=['Gender']), encoded_df], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(columns=['Purchased']),\n",
    "    data['Purchased'],\n",
    "    test_size=0.2,\n",
    "    random_state = 42,\n",
    "    stratify = data['Purchased'] #Making sure that train and test data has equal percentage of purchases\n",
    ")\n",
    "    \n",
    "#Now we need to apply the feature scaling\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddda60e-e765-4cfa-8500-f95ca559d236",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11a764b3-e744-4239-ab14-1e68f98607cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "parameters_grid = {\n",
    "    'C' : [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'solver' : ['saga', 'liblinear'],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "}\n",
    "reg = LogisticRegression(max_iter = 1000)\n",
    "grid_search = GridSearchCV(reg, parameters_grid, cv = 8, n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_logi_reg = grid_search.best_estimator_\n",
    "y_pred_logi_reg = best_logi_reg.predict(X_test)\n",
    "\n",
    "#K Nearest Neighbours\n",
    "parameters_grid = {\n",
    "    'n_neighbors': np.arange(2, 10, 1),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan'], \n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search_knn = GridSearchCV(knn, parameters_grid, cv = 8, n_jobs = -1)\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "best_knn = grid_search_knn.best_estimator_\n",
    "y_pred_knn = best_knn.predict(X_test)\n",
    "\n",
    "\n",
    "#Naive Gaussian Bayes\n",
    "gauss_nb = GaussianNB()\n",
    "gauss_nb.fit(X_train, y_train)\n",
    "y_pred_gauss = gauss_nb.predict(X_test)\n",
    "\n",
    "metrics = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_logi_reg),\n",
    "        \"precision\": precision_score(y_test, y_pred_logi_reg, average=\"weighted\"),\n",
    "        \"recall\": recall_score(y_test, y_pred_logi_reg, average=\"weighted\"),\n",
    "        \"f1_score\": f1_score(y_test, y_pred_logi_reg, average=\"weighted\"),\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_knn),\n",
    "        \"precision\": precision_score(y_test, y_pred_knn, average=\"weighted\"),\n",
    "        \"recall\": recall_score(y_test, y_pred_knn, average=\"weighted\"),\n",
    "        \"f1_score\": f1_score(y_test, y_pred_knn, average=\"weighted\"),\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_gauss),\n",
    "        \"precision\": precision_score(y_test, y_pred_gauss, average=\"weighted\"),\n",
    "        \"recall\": recall_score(y_test, y_pred_gauss, average=\"weighted\"),\n",
    "        \"f1_score\": f1_score(y_test, y_pred_gauss, average=\"weighted\"),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0f7cf30-2452-4986-9509-da85fcad005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logistic Regression': {'accuracy': 0.775, 'precision': 0.7842436974789916, 'recall': 0.775, 'f1_score': 0.7574942791762014}, 'KNN': {'accuracy': 0.7375, 'precision': 0.7347894265232975, 'recall': 0.7375, 'f1_score': 0.7195584635661835}, 'Naive Bayes': {'accuracy': 0.875, 'precision': 0.8741264849755416, 'recall': 0.875, 'f1_score': 0.8739697802197803}}\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9fc86-6c2d-4881-bceb-78a4258afb52",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Performance Comparison\n",
    "\n",
    "Below is a detailed comparison of classification metrics across the three models after hyperparameter tuning:\n",
    "\n",
    "| Model               | Accuracy | Precision | Recall | F1-Score |  \n",
    "|---------------------|----------|-----------|--------|----------|  \n",
    "| Logistic Regression | 77.50%   | 78.42%    | 77.50% | 75.75%   |  \n",
    "| K-Nearest Neighbors | 73.75%   | 73.48%    | 73.75% | 71.96%   |  \n",
    "| **Naïve Bayes**     | **87.50%** | **87.41%** | **87.50%** | **87.40%** |  \n",
    "\n",
    "### Key Observations:\n",
    "1. **Naïve Bayes Dominance**  \n",
    "   - Achieved the highest scores across all metrics (87.5% accuracy)\n",
    "   - Strong performance despite no hyperparameter tuning\n",
    "   - Suggests Gaussian assumptions align well with data distribution\n",
    "\n",
    "2. **Logistic Regression Tradeoffs**  \n",
    "   - Moderate performance (77.5% accuracy)\n",
    "   - 1.4% precision-recall gap indicates slight class prediction imbalance\n",
    "\n",
    "3. **KNN Limitations**  \n",
    "   - Lowest performance (73.75% accuracy)  \n",
    "   - Potential sensitivity to feature scaling or noisy observations\n",
    "\n",
    "*All metrics calculated on a held-out test set (20% of original data).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887bd285-83b6-4886-a8e4-8cf54ad8f282",
   "metadata": {},
   "source": [
    "---\n",
    "## More complex models  \n",
    "### 1. Random Forest\n",
    "- **Algorithm**: Classifies based on majority vote of results done by n decision trees, each of whom is based on bootstrapping and randomly picked features.  \n",
    "- **Tuned Parameters**:  \n",
    "  - `n_estimators`: number of decision trees.\n",
    "  - `criterion`: “gini”, “entropy”, “log_loss”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e44983fd-4b91-441b-a956-62886b1531a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3c6a6-a178-49f8-a29f-16616b3dfe0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
